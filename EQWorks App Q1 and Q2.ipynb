{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f5534080-f158-4b8d-be76-c570a0e628d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This codeblock contains all the import statments and the spark setup, run this before running any other blocks of code.\n",
    "\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, radians, asin, sin, sqrt, cos\n",
    "\n",
    "spark = SparkSession.builder.master('local').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "92cd4aa1-da2e-4ba7-841c-0c78f57f1f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function takes 4 inputs, the name of the file to clean, the list of columns on which we will check for duplicates, a name for the new file, \n",
    "#and a Boolean which controls whether the file writes to a CSV or simply returns the dataframe so that it can be used directly as the input to another function.\n",
    "\n",
    "def clean_data_sample (file_name, list_of_columns_to_check, new_file_name, write_csv):\n",
    "\n",
    "    #checks to see if the inputs given are correct\n",
    "    if file_name.endswith('.csv') and type(list_of_columns_to_check) == list:\n",
    "        \n",
    "        #reads the data file into a dataframe\n",
    "        df = spark.read.options(\n",
    "            header='True', \n",
    "            inferSchema='True', \n",
    "            delimiter=',',\n",
    "        ).csv(os.path.expanduser('~/data/' + file_name))\n",
    "        \n",
    "        #provides a row count so that we can tell how many rows were removed\n",
    "        initial_rows = df.count()\n",
    "        final_df = df.dropDuplicates(list_of_columns_to_check)\n",
    "        final_rows = final_df.count()\n",
    "        \n",
    "        #writes dataframe to csv if true, otherwise shows some rows of the dataframe and returns it so that you can use the dataframe with aditional functions\n",
    "        if write_csv == True:\n",
    "            final_df.write.csv(new_file_name)\n",
    "            print(str(initial_rows - final_rows) + ' rows were dropped due to duplicate/suspicious data')\n",
    "            \n",
    "        else: \n",
    "            final_df.show(10)\n",
    "            print(str(initial_rows - final_rows) + ' rows were dropped due to duplicate/suspicious data')\n",
    "            return final_df\n",
    "    \n",
    "    #prints an error and does nothing else if the function is given the wrong inputs    \n",
    "    else:\n",
    "        print('Error: Please check the file name and columns provided')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "00fca421-0677-40c8-8979-75609c9504ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-------+--------+---------+--------+---------+\n",
      "|    _ID|              TimeSt|Country|Province|     City|Latitude|Longitude|\n",
      "+-------+--------------------+-------+--------+---------+--------+---------+\n",
      "|4516516|2017-06-21 00:00:...|     CA|      ON| Waterloo|43.49347|-80.49123|\n",
      "|4519209|2017-06-21 00:00:...|     CA|      ON|  Hanover| 44.1517| -81.0266|\n",
      "|4518130|2017-06-21 00:00:...|     CA|      ON|   London| 43.0004| -81.2343|\n",
      "|5368841|2017-06-21 00:00:...|     CA|      ON|   Nepean| 45.2778| -75.7563|\n",
      "|4521574|2017-06-21 00:00:...|     CA|      ON|Brantford| 43.1508| -80.2094|\n",
      "|4523455|2017-06-21 00:00:...|     CA|      ON|   London| 43.0091| -81.1765|\n",
      "|4522231|2017-06-21 00:00:...|     CA|      ON|  Chatham| 42.4247| -82.1755|\n",
      "|4522376|2017-06-21 00:00:...|     CA|      ON| Waterloo| 43.4634| -80.5201|\n",
      "|4524947|2017-06-21 00:00:...|     CA|      ON|Kitchener| 43.4306| -80.4877|\n",
      "|4526599|2017-06-21 00:00:...|     CA|      ON| Ancaster|  43.208| -79.9652|\n",
      "+-------+--------------------+-------+--------+---------+--------+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "2026 rows were dropped due to duplicate/suspicious data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[_ID: int,  TimeSt: timestamp, Country: string, Province: string, City: string, Latitude: double, Longitude: double]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#runs the data cleaning function\n",
    "clean_data_sample('DataSample.csv', [' TimeSt', 'Latitude', 'Longitude'], 'DataSampleClean', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "797ad8a0-e52e-4a23-b7b0-c8d734a77d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function takes 4 inputs, the name of the file that needs POIs assigned, the name of the POI file we are using to assign POIs, a name for the new file created, \n",
    "#and a Boolean which controls whether the file writes to a CSV or simply returns the dataframe so that it can be used directly as the input to another function.\n",
    "\n",
    "def assign_POI(data_file_name, POI_file_name, new_file_name, write_csv):\n",
    "    #reads the data file into a dataframe\n",
    "    data_df = spark.read.options(\n",
    "        header='True', \n",
    "        inferSchema='True', \n",
    "        delimiter=',',\n",
    "    ).csv(os.path.expanduser('~/data/DataSample.csv'))\n",
    "    \n",
    "    #reads the POI file into a dataframe\n",
    "    POI_df = spark.read.options(\n",
    "        header='True', \n",
    "        inferSchema='True', \n",
    "        delimiter=',',\n",
    "    ).csv(os.path.expanduser('~/data/POIList.csv'))\n",
    "    \n",
    "    #renames columns so that they dont share a name after a join\n",
    "    POI_df = POI_df.withColumnRenamed(\" Latitude\",\"Latitude2\") \\\n",
    "        .withColumnRenamed(\"Longitude\",\"Longitude2\")\n",
    "\n",
    "    #cross joins the data and POI tables and calculates the haversine distance between points then takes the row with the closest POI \n",
    "    #you can find more info on the haversine function at the end of the notebook\n",
    "    final_df = data_df.crossJoin(POI_df) \\\n",
    "        .withColumn(\"dlong\", radians(col(\"Longitude2\")) - radians(col(\"Longitude\"))) \\\n",
    "        .withColumn(\"dlat\", radians(col(\"Latitude2\")) - radians(col(\"Latitude\"))) \\\n",
    "        .withColumn(\"haversine_dist\", asin(sqrt(sin(col(\"dlat\") / 2) ** 2 + cos(radians(col(\"Latitude\"))) * cos(radians(col(\"Latitude2\"))) * sin(col(\"dlong\") / 2) ** 2)) * 2 * 6371) \\\n",
    "        .drop(\"dlong\", \"dlat\") \\\n",
    "        .groupBy(\"_ID\", \" TimeSt\", \"Country\", \"Province\", \"City\", \"Latitude\", \"Longitude\", \"POIID\") \\\n",
    "        .min(\"haversine_dist\") \\\n",
    "        .drop(\"min(haversine_dist)\")\n",
    "    \n",
    "    #writes dataframe to csv if true, otherwise shows some rows of the dataframe and returns it so that you can use the dataframe with aditional functions\n",
    "    if write_csv == True:\n",
    "        final_df.write.csv(new_file_name)\n",
    "        \n",
    "    else: \n",
    "        final_df.show(10)\n",
    "        return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0fa6c41b-cb6c-482b-b452-b51fb369e257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-------+--------+-------------+--------+---------+-----+\n",
      "|    _ID|              TimeSt|Country|Province|         City|Latitude|Longitude|POIID|\n",
      "+-------+--------------------+-------+--------+-------------+--------+---------+-----+\n",
      "|4517999|2017-06-21 12:00:...|     CA|      ON|    Kitchener| 43.4553| -80.4845| POI1|\n",
      "|4524928|2017-06-21 01:00:...|     CA|      ON|    Kitchener|43.44144|-80.50167| POI1|\n",
      "|4530905|2017-06-21 04:00:...|     CA|      AB|      Calgary| 51.0412|-114.0762| POI4|\n",
      "|4533231|2017-06-21 18:00:...|     CA|      ON|  Mississauga| 43.5769| -79.6283| POI4|\n",
      "|4533920|2017-06-21 22:00:...|     CA|      AB|Sherwood Park| 53.5263| -113.287| POI2|\n",
      "|4534602|2017-06-21 22:00:...|     CA|      AB|     Red Deer| 52.2529| -113.807| POI1|\n",
      "|4535473|2017-06-21 19:00:...|     CA|      BC|Prince George| 53.9448| -122.752| POI3|\n",
      "|4545805|2017-06-21 06:01:...|     CA|      AB|     Red Deer| 52.3116| -113.842| POI3|\n",
      "|4549850|2017-06-21 05:01:...|     CA|      ON|    Brantford|43.14094|-80.26627| POI4|\n",
      "|4555402|2017-06-21 22:01:...|     CA|      AB|      Calgary| 50.9316| -114.096| POI2|\n",
      "+-------+--------------------+-------+--------+-------------+--------+---------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[_ID: int,  TimeSt: timestamp, Country: string, Province: string, City: string, Latitude: double, Longitude: double, POIID: string]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#runs the POI assigning function\n",
    "assign_POI('DataSample.csv', 'POIList.csv', 'DataSamplePOI', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6381585f-43eb-48a5-b590-a52a2fd14fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not for any of the questions, just for further information and a brief explanation of why i choose to solve the POI problem the way i did.\n",
    "\n",
    "#this is a version of the haversine calculation done in the assign_POI function that is easier to read/understand. It takes lat-long coordinates and calculates the distance between them\n",
    "#on a sphere. I chose not to simply apply this function to a new column because user defined functions in spark are typically not as well optimized and as such should be avoided when\n",
    "#optimized spark functions exist that can do the same job.\n",
    "\n",
    "#here is a link to the wikipedia page of the haversine formula. https://en.wikipedia.org/wiki/Haversine_formula\n",
    "\n",
    "import math\n",
    "\n",
    "def haversine_distance (lat1, long1, lat2, long2):\n",
    "    earth_radius = 6371\n",
    "\n",
    "    dif_lat = math.radians(lat2 - lat1)\n",
    "    dif_long = math.radians(long2 - long1)\n",
    "    \n",
    "    a = (math.sin(dif_lat / 2) * math.sin(dif_lat / 2) + math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) * math.sin(dif_long / 2) * math.sin(dif_long / 2))\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    d = earth_radius * c\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759d5deb-765f-4360-9bdc-bfbde06ec540",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
